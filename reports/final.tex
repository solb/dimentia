\documentclass[12pt]{article}
\usepackage{gensymb}
\usepackage[margin=1in]{geometry}

\title{\Large Dimentia: Detecting Logic Errors with Dimensional Analysis}
\author{Goran \v{Z}u\v{z}i\'c and Sol Boucher}

\begin{document}
\maketitle

\section*{Abstract}

\textit{We present \textnormal{Dimentia}, a system that uses static analysis to detect possible program logic errors related to dimensionality.}

\section{Related work}

\subsection{Semantic types}

\subsection{Dimensionality}

\cite{hilfinger1988ada} views variables' units of measure as additional type information; as such, it builds a package allowing the programmer to declare general classes of data (e.g. \texttt{DISTANCE} and \texttt{TIME}) as well as specific units (e.g. \texttt{CM} and \texttt{SEC}).
Variables and constants can then be annotated with these respective bits of metadata.
The author's package requires the programmer to define data classes' multiplicative relationships, which is done by supplying matricies of coefficients.
When it encounters an expression in the program, the package retrieves these matrices by looking up the operands' classes by units.
It then verifies that the program's computations are consistent with this dimensionality information.

\section{Problem}

Although past work in dimensional analysis offers rich semantic checking of programs thanks to its perfect understanding of variables' data classes, this comes with the cost of requiring significant programmer effort.
Data classes must be explicitly defined and related, and every non-dimensionless variable must be explicitly related to a unit.
We observe that, while having such detailed information enables very accurate verification, it is possible to perform dimensional analysis without any additional information about the program or its variables.
This approach shifts the focus from one of directly detecting \textit{correctness} violations to one of finding \textit{consistency} violations: it lacks the information to identify semantic errors when they appear in isolation, but given a larger program, is able to identify conflicting uses of values.
Moreover, because no additional labor is imposed on the programmer, such a system is more likely to be used, especially in situations such as competitive programming, where rapid detection of bugs is important but it's development time that is paramount.

\section{Approach}

We define the \textit{degree} of a variable as the exponent on its units; for instance, lengths (often expressed in cm) would have a degree of 1, whereas areas' (often expressed in cm\textsuperscript{2}) would be 2.
We write $\degree(\tt{x}) = d$ to indicate that the source program variable (or temporary register) \texttt{x} has degree \textit{d}, so if \texttt{l} were a length and \texttt{a} were an area, we might write $\degree(\tt{l}) = 1$ and $\degree(\tt{a}) = 2$.
In processing a program that provides no information about data classes, we aim to deduce the \textit{relative} degrees of variables based on their actual use in the program.
This is done by generating a system of linear equations, each member of which corresponds to an arithmetic, comparison, or memory instruction in the host program.
Here's how we handle basic arithmetic and logic operations:
\\

\begin{tabular}{l l l}
\textbf{Instruction} & \textbf{Equation generated} & \textbf{Matrix representation \texttt{\{z, x, y\}}} \\
\hline \\
\texttt{z = add x, y} & $\degree(\tt{z}) =\ \degree(\tt{x}) =\ \degree(\tt{y})$ & \texttt{\{1, -1, 0\}, \{1, 0, -1\}}\\
\texttt{z = sub x, y} & $\degree(\tt{z}) =\ \degree(\tt{x}) =\ \degree(\tt{y})$ & \texttt{\{1, -1, 0\}, \{1, 0, -1\}}\\
\texttt{z = mul x, y} & $\degree(\tt{z}) =\ \degree(\tt{x}) +\ \degree(\tt{y})$ & \texttt{\{1, -1, -1\}} \\
\texttt{z = div x, y} & $\degree(\tt{z}) =\ \degree(\tt{x}) -\ \degree(\tt{y})$ & \texttt{\{1, -1, 1\}} \\
\texttt{z = cmp x, y} & $\degree(\tt{x}) =\ \degree(\tt{y})$ & \texttt{\{0, 1, -1\}} \\
\end{tabular}
\\

\noindent
At the end of instruction processing, we're left with a single matrix consisting the system of equations relating variables' degrees, and at this point our task is to find out whether each variable was used in an internally consistent manner throughout the program.
We accomplish this by running an SVD solver on the system: the null space of the resulting matrix tells gives the set of \textit{dimensionless} variables, or those to which no single degree could be assigned.
The presence of such variables indicates their involvement in multiple arithmetic or logical operations with variables of differing degree, which is likely a logic error.
The inconsistent instructions can be found by iteratively removing one equation at a time from the matrix and resolving it to see whether any variables disappear from the null space.
While this process is expensive, it only needs to be performed on those equations that include dimensionless variables.

\subsection{Implementation}

\textit{Dimentia} is implemented as an LLVM 3.7 analysis pass; as such, it is invoked using the framework's \texttt{opt} optimization utility and can accept either LLVM IR assembly code or an LLVM bitcode object.
The decision to implement as a pass rather than as part of the language frontend was made to keep it language-agnostic.
In fact, since our method doesn't require programmer annotations, it should be possible to run it unmodified on programs compiled by any LLVM frontend, regardless of source language (although we've only tested with Clang).
However, since our aim is to detect errors in the source program, we must be able relate registers back to their high-level variables and map instructions to the source lines that generated them; for this, we use debugging annotations generated by the frontend, and require that the user has compiled in debug mode.

\textit{Dimentia} actually consists of two separate passes, but the first is solely responsible for processing debugging information: it builds lookup tables that bidirectionally associate registers with source program variables.
To do this, it walks the \texttt{globals} section of the \texttt{llvm.dbg.cu} compilation unit debugging metadata, then searches for \texttt{llvm.dbg.declare} and \texttt{llvm.dbg.value} intrinsic instructions in the body of the program.
After the pass completes, these lookup tables are provided to a second pass that's responsible for the actual dimensional analysis.

The bulk of the work done by this pass is in examining all of the program's instructions and generating corresponding degree equations.
At the beginning of the program, a column is allocated for each source program variable, as enumerated by the first pass.
Thereafter, as each instruction is processed, entries for operands that map directly to source program variables are placed directly in the columns for those variables in order to keep the size of the matrices down.
Whenever an instruction involves a temporary register, the pass checks whether a column has been allocated for it.
If so, the column is reused; otherwise, a new one is created and this action recorded so future instructions are aware of it.
Each equations is stored in a variable-width array because the presence of temporary registers means that the number of columns isn't known until all instructions have been processed.

While this is all that's required to handle scalar types, vector types pose challenges of their own.
In order to accommodate arrays and structured types, we handle the \texttt{getelementptr}, \texttt{load}, and \texttt{store} instructions specially.
The former instruction creates a matrix column (and hence, common degree) to be shared among accesses of that \textit{class} of equivalent locations.
For arrays, we make the assumption that the same sort of data is stored throughout, so we define their class as all elements of that same array.
For structured types, the semantic relationship is cross-sectional, and we define the class of a particular element as that same element of any instance of the same struct.
These column redirections are stored in a separate table that is consulted only when processing the pointer operand of \texttt{load} and \texttt{store} instructions.

With all instructions processed and the individual equations produced, we resize each by adding zeroes at the end until the system forms a non-ragged matrix.
We then pass this to the LAPACK FORTRAN SVD implementation to obtain the set of dimensionless variables.
We then use the approach described above, combined with a final consultation of the program's debugging annotations, to identify potentially problematic lines and trace them back to source program locations that should be presented to the user.

\section{Evaluation}

\section{Results}

\section{Conclusion}

\section{Future work}

\bibliographystyle{abbrv}
\bibliography{refs}

\end{document}
