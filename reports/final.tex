\documentclass[12pt]{article}
\usepackage{gensymb}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{color}
\usepackage{listings}
\usepackage{cleveref}
\usepackage{float}


\lstset{language=C++,
  basicstyle=\ttfamily,
  keywordstyle=\color{blue}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  commentstyle=\color{green}\ttfamily,
  morecomment=[l][\color{magenta}]{\#},
  moredelim=**[is][\color{red}]{@}{@}
}

\title{\Large Dimentia: Detecting Logic Errors with Dimensional Analysis}
\author{Goran \v{Z}u\v{z}i\'c and Sol Boucher}

\begin{document}
\maketitle

\section*{Abstract}

\textit{We present \textnormal{Dimentia}, a system that uses static analysis to detect possible program logic errors related to dimensionality.}

\section{Introduction}

Logical bugs are a big source of programming errors and can be notoriously hard to catch. While there is no hope to detect all logical mistakes in source code, one might hope to design bug detection software to capture a specific type of probable bugs. To this end, we propose a static bug detection technique that uses dimensional analysis to flag possible programmer mistakes.

Examine the C++ source code in \Cref{code:example1}. If we remove the statement ``\texttt{/ sq\_dist(...)}'' marked in red, the code contains a dimensional bug. To see the bug, it is important to know that the variables \texttt{v, g} and \texttt{pts[.]} hold 2D points. Let's denote the dimension of the unit distance of this 2D plane as $E$. It can be inferred that the dimension of \texttt{dot\_prod} is $E^2$. Without the statement in red, the dimension of \texttt{alpha} is also $E^2$. But this implies that \texttt{pts[i].x + alpha*v.x} will be adding together variables of dimension $E$ and $E^3$, a clear dimensional bug. The proposed method vows to catch such bugs.

\begin{lstlisting}[label=code:example1,caption=Example of a dimensional bug]
  double dot_prod = (g.x-pts[i].x)*v.x + (g.y-pts[i].y)*v.y;
  double alpha = dot_prod @/ sq_dist(v, {0, 0})@;
  if (...) {
    Pt cand = {pts[i].x + alpha*v.x, pts[i].y + alpha*v.y};
    ...
  }
\end{lstlisting}

This example from \Cref{code:example1} comes from a competitive-programming event (edited for clarity) and this was the actual bug one of the authors made during the contest. % reference?

\section{Related work}

Checking dimensional consistency is a classical topic in software analysis. Intermixing physical units with native program code and utilizing dimensionality checking appeared in Ada~\cite{hilfinger1988ada} and Pascal~\cite{dreiheller1986programming, gehani1977units}. The common theme in these approaches is defining a language extension that will associate an expressive physical unit to a variable and then using rule-based inference to catch dimensionality bugs. 

For instance, \cite{hilfinger1988ada} builds a package allowing the programmer to declare general classes of data (e.g. \texttt{DISTANCE} and \texttt{TIME}) as well as specific units (e.g. \texttt{CM} and \texttt{SEC}). Variables and constants can then be annotated with these respective bits of metadata. The author's package requires the programmer to define data classes' multiplicative relationships, which is done by supplying matricies of coefficients.
When it encounters an expression in the program, the package retrieves these matrices by looking up the operands' classes by units. It then verifies that the program's computations are consistent with this dimensionality information.

Although past work in dimensional analysis offers rich semantic checking of programs thanks to its perfect understanding of variables' data classes, this comes with the cost of requiring significant programmer effort. Data classes must be explicitly defined and related, and every non-dimensionless variable must be explicitly related to a unit.

We observe that, while having such detailed information enables very accurate verification, it is possible to perform dimensional analysis without any additional information about the program or its variables.

This approach shifts the focus from one of directly detecting \textit{correctness} violations to one of finding \textit{consistency} violations: it lacks the information to identify semantic errors when they appear in isolation, but given a larger program, is able to identify conflicting uses of values. Moreover, because no additional labor is imposed on the programmer, such a system is more likely to be used, especially in situations such as competitive programming, where rapid detection of bugs is important but it's development time that is paramount.

\section{Our Approach}
\label{sec:approach}

In order to associate a dimension with a variable (or more generally, a semantic type), we define the scalar \textit{degree} of a semantic type as the sum of exponents on its units; for instance, lengths (often expressed in meters) would have a degree of $1$, areas' (often expressed in meters squared) would be $2$ and acceleration (often expressed in meters per seconds square, $[m s^{-2}]$) would be $-1$.

We write $\degree(\tt{x}) = d$ to indicate that the type associated with the source program variable (or temporary register) \texttt{x} has degree \textit{d}, so if \texttt{l} were a length and \texttt{a} were an area, we might write $\degree(\tt{l}) = 1$ and $\degree(\tt{a}) = 2$.
In processing a program that provides no information about data classes, we aim to deduce the degrees of variables based on their actual use in the program.

This is done by establishing consistency rules that must be satisfied when a mathematical operation is applied between variables. For instance, an instruction $z := x + y$ only makes sense if $\degree{\tt{z}} = \degree{\tt{x}} = \degree{\tt{y}}$. The implemented list of rules is shown in \Cref{table:dimensional-rules}.

\begin{table}[H]
  \centering
  \setlength{\tabcolsep}{20pt}
  \begin{tabular}{c | c}
    \textbf{Instruction} & \textbf{Equation generated} \\
    \hline
    $z := x + y$ & $\degree(\tt{z}) =\ \degree(\tt{x}) =\ \degree(\tt{y})$ \\
    $z := x - y$ & $\degree(\tt{z}) =\ \degree(\tt{x}) =\ \degree(\tt{y})$ \\
    $z := x \cdot y$ & $\degree(\tt{z}) =\ \degree(\tt{x}) +\ \degree(\tt{y})$ \\
    $z := x / y$ & $\degree(\tt{z}) =\ \degree(\tt{x}) -\ \degree(\tt{y})$ \\
    $x < y$ & $\degree(\tt{x}) =\ \degree(\tt{y})$ \\
  \end{tabular}
  \caption{Rules for degree consistency}
  \label{table:dimensional-rules}
\end{table}

Note that each of rules is essentially a linear equation between the arguments' degrees. At the end of instruction processing, we are left with a single matrix consisting the system of equations relating variables' degrees, and at this point our task is to find out whether each variable was used in an internally consistent manner throughout the program.

By denoting the matrix by $A$ and the list of degrees by $x$, our consistency rules furnish a linear system $Ax = 0$. Note that the system is homogenous, so $x = 0$ is always a valid (consistent) solution. In order to identify a potential bug, we define a \textbf{dimensionless type}, a type that can not have a non-zero degree without violating the constraints. For instance, a instruction of the form $z = z \cdot a$ would imply that the type of $a$ is dimensionless (must have degree 0).

The presence of such variables (with dimensionless types) indicates there is no consistent way to assign a nontrivial unit to them, thereby indicating a possible logic error.

While the described approach can help us find potentially inconsistent variables, it doesn't directly help up pinpoint the locations of possible bugs in the program. To this end, we flag a source code line as potentially unconsistent if removing it from the program would reduce the numebr of dimensionless types.

\subsection{Implementation}

We created the tool \textit{Dimentia} that follows the approach described in \Cref{sec:approach}. It is implemented as an LLVM 3.7 analysis pass; as such, it is invoked using the framework's \texttt{opt} optimization utility and can accept either LLVM IR assembly code or an LLVM bitcode object.
The decision to implement as a pass rather than as part of the language frontend was made to keep it language-agnostic.
In fact, since our method doesn't require programmer annotations, it should be possible to run it unmodified on programs compiled by any LLVM frontend, regardless of source language (although we've only tested with Clang).
However, since our aim is to detect errors in the source program, we must be able relate registers back to their high-level variables and map instructions to the source lines that generated them; for this, we use debugging annotations generated by the frontend, and require that the user has compiled in debug mode.

\textit{Dimentia} actually consists of two separate passes, but the first is solely responsible for processing debugging information: it builds lookup tables that bidirectionally associate registers with source program variables.
To do this, it walks the \texttt{globals} section of the \texttt{llvm.dbg.cu} compilation unit debugging metadata, then searches for \texttt{llvm.dbg.declare} and \texttt{llvm.dbg.value} intrinsic instructions in the body of the program.
After the pass completes, these lookup tables are provided to a second pass that's responsible for the actual dimensional analysis.

The bulk of the work done by this pass is in examining all of the program's instructions and generating corresponding degree equations.
At the beginning of the program, a column is allocated for each source program variable, as enumerated by the first pass.
Thereafter, as each instruction is processed, entries for operands that map directly to source program variables are placed directly in the columns for those variables in order to keep the size of the matrices down.
Whenever an instruction involves a temporary register, the pass checks whether a column has been allocated for it.
If so, the column is reused; otherwise, a new one is created and this action recorded so future instructions are aware of it.
Each equations is stored in a variable-width array because the presence of temporary registers means that the number of columns isn't known until all instructions have been processed.

While this is all that's required to handle scalar types, vector types pose challenges of their own.
In order to accommodate arrays and structured types, we handle the \texttt{getelementptr}, \texttt{load}, and \texttt{store} instructions specially.
The former instruction creates a matrix column (and hence, common degree) to be shared among accesses of that \textit{class} of equivalent locations.
For arrays, we make the assumption that the same sort of data is stored throughout, so we define their class as all elements of that same array.
For structured types, the semantic relationship is cross-sectional, and we define the class of a particular element as that same element of any instance of the same struct.
These column redirections are stored in a separate table that is consulted only when processing the pointer operand of \texttt{load} and \texttt{store} instructions.

With all instructions processed and the individual equations produced, we resize each by adding zeroes at the end until the system forms a non-ragged matrix. We now describe the implementation of detecting dimensionless variables. If we denote the matrix as $A$ and the degree vector as $x$, we have a homogenous linear system $Ax = 0$. Thus, every element of the nullspace $N(A)$ is a valid assignment to the degree vector $x$. Hence in order to find a variable at index $i$ that is dimensionless, we have to check if every vector of the nullspace has a $0$ at the index $i$. Note that it is sufficient to check any spanning set of vectors of the nullspace for that condition.

We are left with finding a spanning set for the nullspace. But this can be straightforwardly achieved via a singular value decomposition (SVD). We used the SVD implementation given in Lapack 3. This concludes the our method of flagging dimensionless variables.

The final ingredient in our approach, identification of potentially non-consistent source lines is done in a straightforward manner: for each source line we remove all the linear equations generated by that source line and re-run the analysis. If the number of dimensionless types decreases, we flag the line.

We then use the approach described above, combined with a final consultation of the program's debugging annotations, to identify potentially variables and problematic lines and trace them back to source program locations that should be presented to the user.

\subsection{Limitations}

\textit{Dimentia} presently suffers from a few noteworthy limitations.

\begin{itemize}
\item \textbf{Unhandled arithmetic operations.}
We don't generate equations for modulus (remainder) operations or bitwise arithmetic because it's non-obvious what effect these operations have on variables' degree relationships.

\item \textbf{Pointers and pointer arithmetic.}
While we can handle memory accesses directly into arrays and structures, we don't perform special tracking of pointers or their modification.
For this reason, performing pointer arithmetic prior to a dereference may produce unexpected results.

\item \textbf{Functions, arguments, and return values.}
We don't presently track the associations between call sites and functions, arguments, and parameters, or return statements and call evaluations.
This means that, although we can detect dimensional errors within individual functions, we cannot tell whether a function is misused by client code.

\item \textbf{Platform specificity.}
The current implementation uses LLVM object addresses as hash codes as part of the process of mapping a value to the matrix column storing information about its degree.
While this model works without hackery on source variables, temporaries, and arrays---we simply use the address of the corresponding \texttt{DIVariable}, \texttt{Value}, or \texttt{DIVariable} of the entire array, respectively---it requires some tweaking in order to accommodate the cross-sectional lookups necessary for associating the fields of different struct instances.
To handle this case, we start with the \texttt{StructType} object describing the global definition of that struct type; however, since there are no global LLVM objects corresponding to the individual positions \textit{within} the struct, we cannot simply use an address.
Instead, we pack the offset into the struct into the top sixteen bits of the address, which works because x86-64 machines have a 64-bit word size but only a 48-bit address bus.
Of course, other platforms may use these bits of pointers, and in such a case, this approach could result in hash and equality collisions.

\item \textbf{Oversized structure offsets.}
As stated above, we pack structure offsets into 16 bits, which is likely to be too small for particularly large types.

\item \textbf{Arrays within structures.}
In order to find the offset into a struct for the purpose of associating an access with the appropriate degree class, we need the offset into that structure to be a compile-time constant.
Unfortunately, when an array appears within a structure, this is not the case, so we're unable to compute the hash codes for such accesses.
While it would be possible to compute the partial constants of accesses in order to collapse all accesses within the array element down, this would complicate the code for handling memory operations, and is currently not implemented.
\end{itemize}

\section{Evaluation}

We evaluated the tool on a set of source codes stemming from competitive programming. Some of the examples were hand picked and other were randomly selected. The sour

\section{Results}

\section{Conclusion}

\section{Future work}

\bibliographystyle{abbrv}
\bibliography{refs}

\end{document}
